{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/mlg-ulb/creditcardfraud/kernels\n",
    "# Credit Card Fraud Detection\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('creditcard.csv')\n",
    "df= df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([47488.0625855]), array([94813.85957508]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize the time feature and amount feature.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit( np.array( df[ \"Time\"]).reshape(-1, 1) )\n",
    "scaler.scale_, scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit( np.array( df[ \"Time\"]).reshape(-1, 1) )\n",
    "df[\"Time\"]= scaler.transform( np.array( df[ \"Time\"]).reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_amount = StandardScaler()\n",
    "scaler.fit( np.array( df[ \"Amount\"]).reshape(-1, 1) )\n",
    "df[\"Amount\"]= scaler.transform( np.array( df[ \"Amount\"]).reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Class\"]= df[\"Class\"].apply( lambda x: 1 if x==1 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -1.996583 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1 -1.996583  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2 -1.996562 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3 -1.996562 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4 -1.996541 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  0.244964     -1  \n",
       "1  0.167170  0.125895 -0.008983  0.014724 -0.342475     -1  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  1.160686     -1  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  0.140534     -1  \n",
       "4 -0.206010  0.502292  0.219422  0.215153 -0.073403     -1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the missing data\n",
    "missing_data= np.sum( df.isnull(), axis=0 )\n",
    "missing_features=[ feature for feature in missing_data.index if missing_data[feature]>0 ]\n",
    "missing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEqVJREFUeJzt3X+sX3ddx/Hni5YhimPFlTHahU4t0bnIGHUsEM2UuHVLTAGZDiJrcLFIOhWjxkFiRsAlmvBDB7M4XFlHkLIwcTUWax1TJAzoHTT7Kdl1TnZZXTs6tykZsvH2j+/nynfdt7ff2/Vzv/X2+UhOvuf7Pp/zOZ+TNHn1nPP5npuqQpKknp416QFIkhY/w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKm7pZMewNHixBNPrFWrVk16GJL0/8qtt976UFUtP1Q7w6ZZtWoVU1NTkx6GJP2/kuTfx2nnbTRJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUne+QeAImvrNX5/0EHQUWnPlhyc9BGnivLKRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO66hU2SU5LcnOTuJHcm+a1Wf1eSbyTZ3ZYLhvZ5R5LpJF9Lct5QfW2rTSe5bKh+apIvJbknySeTHNfqz2nfp9v2Vb3OU5J0aD2vbJ4Afqeqfhw4G9iY5LS27QNVdUZbtgO0bRcBPwGsBf4syZIkS4CrgPOB04A3DvXzx62v1cDDwCWtfgnwcFX9KPCB1k6SNCHdwqaq9lTVV9r6Y8DdwIo5dlkHbK2qb1fVvwHTwFltma6qe6vqf4CtwLokAX4O+FTbfwvw2qG+trT1TwGvae0lSROwIM9s2m2slwNfaqVLk9yWZHOSZa22Arh/aLeZVjtY/YeA/6yqJw6oP6Wvtv2R1l6SNAHdwybJ84AbgLdX1aPAJuBHgDOAPcD7ZpuO2L0Ooz5XXweObUOSqSRT+/btm/M8JEmHr2vYJHk2g6D5eFX9FUBVPVhVT1bVd4GPMLhNBoMrk1OGdl8JPDBH/SHghCRLD6g/pa+2/fnA/gPHV1VXV9WaqlqzfPnyZ3q6kqSD6DkbLcA1wN1V9f6h+slDzV4H3NHWtwEXtZlkpwKrgS8Du4DVbebZcQwmEWyrqgJuBt7Q9l8P3DjU1/q2/gbgs629JGkClh66yWF7NfBm4PYku1vtnQxmk53B4LbWfcBbAarqziTXA3cxmMm2saqeBEhyKbADWAJsrqo7W3+/D2xN8ofAVxmEG+3zY0mmGVzRXNTxPCVJh9AtbKrq84x+drJ9jn2uAK4YUd8+ar+qupfv3YYbrj8OXDif8UqS+vENApKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqbtuYZPklCQ3J7k7yZ1JfqvVX5BkZ5J72ueyVk+SK5NMJ7ktyZlDfa1v7e9Jsn6o/ookt7d9rkySuY4hSZqMnlc2TwC/U1U/DpwNbExyGnAZcFNVrQZuat8BzgdWt2UDsAkGwQFcDrwSOAu4fCg8NrW2s/utbfWDHUOSNAHdwqaq9lTVV9r6Y8DdwApgHbClNdsCvLatrwOuq4EvAickORk4D9hZVfur6mFgJ7C2bTu+qm6pqgKuO6CvUceQJE3AgjyzSbIKeDnwJeCkqtoDg0ACXtiarQDuH9ptptXmqs+MqDPHMSRJE9A9bJI8D7gBeHtVPTpX0xG1Ooz6fMa2IclUkql9+/bNZ1dJ0jx0DZskz2YQNB+vqr9q5QfbLTDa595WnwFOGdp9JfDAIeorR9TnOsZTVNXVVbWmqtYsX7788E5SknRIPWejBbgGuLuq3j+0aRswO6NsPXDjUP3iNivtbOCRdgtsB3BukmVtYsC5wI627bEkZ7djXXxAX6OOIUmagKUd+3418Gbg9iS7W+2dwB8B1ye5BPg6cGHbth24AJgGvgW8BaCq9id5D7CrtXt3Ve1v628DrgWeC3ymLcxxDEnSBHQLm6r6PKOfqwC8ZkT7AjYepK/NwOYR9Sng9BH1b446hiRpMnyDgCSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1N1bYJLlpnJokSaMsnWtjku8Dvh84MckyIG3T8cCLO49NkrRIzBk2wFuBtzMIllv5Xtg8ClzVcVySpEVkzrCpqj8F/jTJb1TVBxdoTJKkReZQVzYAVNUHk7wKWDW8T1Vd12lckqRFZKywSfIx4EeA3cCTrVyAYSNJOqSxwgZYA5xWVdVzMJKkxWnc39ncAbyo50AkSYvXuGFzInBXkh1Jts0uc+2QZHOSvUnuGKq9K8k3kuxuywVD296RZDrJ15KcN1Rf22rTSS4bqp+a5EtJ7knyySTHtfpz2vfptn3VmOcoSepk3Nto7zqMvq8FPsTTn+t8oKreO1xIchpwEfATDKZZ/0OSl7bNVwE/D8wAu5Jsq6q7gD9ufW1N8mHgEmBT+3y4qn40yUWt3S8fxvglSUfIuLPR/mm+HVfV5+ZxVbEO2FpV3wb+Lck0cFbbNl1V9wIk2QqsS3I38HPAm1qbLQwCcVPr612t/ingQ0ni8yZJmpxxX1fzWJJH2/J4kieTPHqYx7w0yW3tNtuyVlsB3D/UZqbVDlb/IeA/q+qJA+pP6attf6S1lyRNyFhhU1U/WFXHt+X7gF9kcItsvjYxmEJ9BrAHeF+rZ0TbOoz6XH09TZINSaaSTO3bt2+ucUuSnoHDeutzVf01g9tY893vwap6sqq+C3yE790qmwFOGWq6EnhgjvpDwAlJlh5Qf0pfbfvzgf0HGc/VVbWmqtYsX758vqcjSRrTuD/qfP3Q12cx+N3NvJ+BJDm5qva0r69jMKUaYBvwl0nez2CCwGrgywyuUlYnORX4BoNJBG+qqkpyM/AGYCuwHrhxqK/1wC1t+2d9XiNJkzXubLRfGFp/AriPwYP4g0ryCeAcBm+MngEuB85JcgaDoLqPwYs+qao7k1wP3NX631hVT7Z+LgV2AEuAzVV1ZzvE7wNbk/wh8FXgmla/BvhYm2Swn0FASZImaNzZaG+Zb8dV9cYR5WtG1GbbXwFcMaK+Hdg+on4v37sNN1x/HLhwXoOVJHU17my0lUk+3X6k+WCSG5Ks7D04SdLiMO4EgY8yeBbyYgZTi/+m1SRJOqRxw2Z5VX20qp5oy7WA07ckSWMZN2weSvIrSZa05VeAb/YcmCRp8Rg3bH4V+CXgPxj8GPMNwLwnDUiSjk3jTn1+D7C+qh4GSPIC4L0MQkiSpDmNe2Xzk7NBA1BV+4GX9xmSJGmxGTdsnjX00szZK5txr4okSce4cQPjfcAXknyKwa//f4kRP8CUJGmUcd8gcF2SKQYv3wzw+vYHzCRJOqSxb4W1cDFgJEnzdlh/YkCSpPkwbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR11y1skmxOsjfJHUO1FyTZmeSe9rms1ZPkyiTTSW5LcubQPutb+3uSrB+qvyLJ7W2fK5NkrmNIkian55XNtcDaA2qXATdV1WrgpvYd4HxgdVs2AJtgEBzA5cArgbOAy4fCY1NrO7vf2kMcQ5I0Id3Cpqo+B+w/oLwO2NLWtwCvHapfVwNfBE5IcjJwHrCzqvZX1cPATmBt23Z8Vd1SVQVcd0Bfo44hSZqQhX5mc1JV7QFony9s9RXA/UPtZlptrvrMiPpcx5AkTcjRMkEgI2p1GPX5HTTZkGQqydS+ffvmu7skaUwLHTYPtltgtM+9rT4DnDLUbiXwwCHqK0fU5zrG01TV1VW1pqrWLF++/LBPSpI0t4UOm23A7Iyy9cCNQ/WL26y0s4FH2i2wHcC5SZa1iQHnAjvatseSnN1moV18QF+jjiFJmpClvTpO8gngHODEJDMMZpX9EXB9kkuArwMXtubbgQuAaeBbwFsAqmp/kvcAu1q7d1fV7KSDtzGY8fZc4DNtYY5jSJImpFvYVNUbD7LpNSPaFrDxIP1sBjaPqE8Bp4+of3PUMSRJk3O0TBCQJC1iho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSepuImGT5L4ktyfZnWSq1V6QZGeSe9rnslZPkiuTTCe5LcmZQ/2sb+3vSbJ+qP6K1v902zcLf5aSpFmTvLL52ao6o6rWtO+XATdV1WrgpvYd4HxgdVs2AJtgEE7A5cArgbOAy2cDqrXZMLTf2v6nI0k6mKPpNto6YEtb3wK8dqh+XQ18ETghycnAecDOqtpfVQ8DO4G1bdvxVXVLVRVw3VBfkqQJmFTYFPD3SW5NsqHVTqqqPQDt84WtvgK4f2jfmVabqz4zov40STYkmUoytW/fvmd4SpKkg1k6oeO+uqoeSPJCYGeSf5mj7ajnLXUY9acXq64GrgZYs2bNyDaSpGduIlc2VfVA+9wLfJrBM5cH2y0w2ufe1nwGOGVo95XAA4eorxxRlyRNyIKHTZIfSPKDs+vAucAdwDZgdkbZeuDGtr4NuLjNSjsbeKTdZtsBnJtkWZsYcC6wo217LMnZbRbaxUN9SZImYBK30U4CPt1mIy8F/rKq/i7JLuD6JJcAXwcubO23AxcA08C3gLcAVNX+JO8BdrV2766q/W39bcC1wHOBz7RFkjQhCx42VXUv8LIR9W8CrxlRL2DjQfraDGweUZ8CTn/Gg5UkHRFH09RnSdIiZdhIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3S3asEmyNsnXkkwnuWzS45GkY9miDJskS4CrgPOB04A3JjltsqOSpGPXogwb4Cxguqrurar/AbYC6yY8Jkk6Zi2d9AA6WQHcP/R9BnjlhMYiTdyvf2Fq0kPQUejDr1qzYMdarGGTEbV6WqNkA7Chff2vJF/rOqpjy4nAQ5MexFHhg38+6RHoqfy32Ryhf5kvGafRYg2bGeCUoe8rgQcObFRVVwNXL9SgjiVJpqpq4f7bJI3Jf5uTsVif2ewCVic5NclxwEXAtgmPSZKOWYvyyqaqnkhyKbADWAJsrqo7JzwsSTpmLcqwAaiq7cD2SY/jGObtSR2t/Lc5Aal62nNzSZKOqMX6zEaSdBQxbHREJfmxJLck+XaS3530eKRZSTYn2ZvkjkmP5Vhk2OhI2w/8JvDeSQ9EOsC1wNpJD+JYZdjoiKqqvVW1C/jOpMciDauqzzH4z5AmwLCRJHVn2EiSujNs9Iwl2Zhkd1tePOnxSDr6LNofdWrhVNVVDP5+kCSN5I86dUQleREwBRwPfBf4L+C0qnp0ogPTMS/JJ4BzGLz1+UHg8qq6ZqKDOoYYNpKk7nxmI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG2kCkrwoydYk/5rkriTbk7zUNxJrsfJHndICSxLg08CWqrqo1c4ATprowKSOvLKRFt7PAt+pqg/PFqpqN3D/7Pckq5L8c5KvtOVVrX5yks+1VwPdkeSnkyxJcm37fnuS3174U5Lm5pWNtPBOB249RJu9wM9X1eNJVgOfANYAbwJ2VNUVSZYA3w+cAayoqtMBkpzQb+jS4TFspKPTs4EPtdtrTwIvbfVdwOYkzwb+uqp2J7kX+OEkHwT+Fvj7iYxYmoO30aSFdyfwikO0+W0G7+96GYMrmuPg//4A2M8A3wA+luTiqnq4tftHYCPwF32GLR0+w0ZaeJ8FnpPk12YLSX4KeMlQm+cDe6rqu8CbgSWt3UuAvVX1EeAa4MwkJwLPqqobgD8AzlyY05DG5200aYFVVSV5HfAnSS4DHgfuA94+1OzPgBuSXAjcDPx3q58D/F6S7zB4o/bFwArgo0lm//P4ju4nIc2Tb32WJHXnbTRJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTu/heaIEXk5yxKdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#unitary data analysis\n",
    "#2.1. Data analysis: unitary data analysis\n",
    "sns.countplot(x='Class',data=df, palette='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of fraud 0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "num_fraud=np.sum( df[\"Class\"]== 1 )\n",
    "num_non_fraud=np.sum( df[\"Class\"]== -1 )\n",
    "print(\"fraction of fraud \"+ str(num_fraud/ (num_non_fraud+ num_fraud)  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X= df.loc[:, df.columns !='Class'];\n",
    "y= df.loc[:, 'Class'];\n",
    "\n",
    "X_train, X_tem, y_train, y_tem= train_test_split(X, y, test_size=0.5, random_state=0);\n",
    "X_val, X_test, y_val, y_test= train_test_split(X_tem, y_tem, test_size=0.6, random_state=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142403, 56961, 85443)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  284308\n",
      "Number of no subscription in oversampled data 142154\n",
      "Number of subscription 142154\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  0.5\n",
      "length of oversampled data is  113730\n",
      "Number of no subscription in oversampled data 56865\n",
      "Number of subscription 56865\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  0.5\n"
     ]
    }
   ],
   "source": [
    "#Make X_train, y_train balanced\n",
    "os=SMOTE(random_state=0);\n",
    "os_X_train, os_y_train= os.fit_sample(X_train, y_train);\n",
    "os_X_train= pd.DataFrame( data= os_X_train, columns= X_train.columns );\n",
    "os_y_train= pd.DataFrame( data= os_y_train, columns=['Class'] );\n",
    "\n",
    "\n",
    "os_X_val, os_y_val= os.fit_sample(X_val, y_val);\n",
    "os_X_val= pd.DataFrame( data= os_X_val, columns= X_val.columns );\n",
    "os_y_val= pd.DataFrame( data= os_y_val, columns=['Class'] );\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_X_train))\n",
    "print(\"Number of no subscription in oversampled data\", sum(os_y_train['Class']==-1) );\n",
    "print(\"Number of subscription\",sum(os_y_train['Class']==1) );\n",
    "print(\"Proportion of no subscription data in oversampled data is \",sum(os_y_train['Class']==-1)/len(os_X_train) )\n",
    "print(\"Proportion of subscription data in oversampled data is \",sum(os_y_train['Class']==1)/len(os_X_train) )\n",
    "\n",
    "print(\"length of oversampled data is \",len(os_X_val))\n",
    "print(\"Number of no subscription in oversampled data\", sum(os_y_val['Class']==-1) );\n",
    "print(\"Number of subscription\",sum(os_y_val['Class']==1) );\n",
    "print(\"Proportion of no subscription data in oversampled data is \",sum(os_y_val['Class']==-1)/len(os_X_val) )\n",
    "print(\"Proportion of subscription data in oversampled data is \",sum(os_y_val['Class']==1)/len(os_X_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9265541194056098\n",
      "[[55464  1401]\n",
      " [   12    84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.98      0.99     56865\n",
      "           1       0.06      0.88      0.11        96\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     56961\n",
      "   macro avg       0.53      0.93      0.55     56961\n",
      "weighted avg       1.00      0.98      0.99     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Start to train the model from discovering the best class_weight\n",
    "#Non-fraud and fraud classes have the same weight in loss function\n",
    "C=1\n",
    "clf = LogisticRegression( penalty='l2', tol=0.0001, C=C, fit_intercept=True, solver='liblinear', max_iter=1000)\n",
    "clf.fit(os_X_train, os_y_train)\n",
    "y_val_pre= clf.predict(os_X_val)\n",
    "print( accuracy_score(y_val_pre, os_y_val ) )  \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_val_pred= clf.predict(X_val)\n",
    "confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8993053723731645\n",
      "[[56848    17]\n",
      " [   21    75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     56865\n",
      "           1       0.82      0.78      0.80        96\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56961\n",
      "   macro avg       0.91      0.89      0.90     56961\n",
      "weighted avg       1.00      1.00      1.00     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reduce the weight of fraud class in the loss function to reduce the false positive predictions\n",
    "#The best class_weight \n",
    "C=1\n",
    "clf = LogisticRegression( penalty='l2', tol=0.0001, C=C, class_weight={-1: 1, 1: 0.01},fit_intercept=True, solver='liblinear', max_iter=1000)\n",
    "clf.fit(os_X_train, os_y_train)\n",
    "y_val_pre= clf.predict(os_X_val)\n",
    "print( accuracy_score(y_val_pre, os_y_val ) ) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_val_pred= clf.predict(X_val)\n",
    "confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744561681174712\n",
      "[[56859     6]\n",
      " [   47    49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     56865\n",
      "           1       0.89      0.51      0.65        96\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     56961\n",
      "   macro avg       0.95      0.76      0.82     56961\n",
      "weighted avg       1.00      1.00      1.00     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Keep reduce the weight of fraud class to 0.001, the number of false negative increases dramatically.\n",
    "C=1\n",
    "clf = LogisticRegression( penalty='l2', tol=0.0001, C=C, class_weight={-1: 1, 1: 0.001},fit_intercept=True, solver='liblinear', max_iter=1000)\n",
    "clf.fit(os_X_train, os_y_train)\n",
    "y_val_pre= clf.predict(os_X_val)\n",
    "print( accuracy_score(y_val_pre, os_y_val ) ) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_val_pred= clf.predict(X_val)\n",
    "confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So that class_weight={-1: 1, 1: 0.01} is the best for this dataset. Let's fix the weights and \n",
    "#        find the best C: the penaty strength.\n",
    "Cs=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]\n",
    "accuracies=[]    \n",
    "for C in Cs:\n",
    "    clf = LogisticRegression( penalty='l1', tol=0.0001, C=C,class_weight={-1: 1, 1: 0.01}, fit_intercept=True, solver='liblinear', max_iter=1000)\n",
    "    clf.fit(os_X_train, os_y_train)\n",
    "    y_val_pre= clf.predict(os_X_val)\n",
    "    accuracies.append(  accuracy_score(y_val_pre, os_y_val ) )\n",
    "    \n",
    "for C in Cs:\n",
    "    clf = LogisticRegression( penalty='l2', tol=0.0001, C=C, class_weight={-1: 1, 1: 0.01}, fit_intercept=True, solver='liblinear', max_iter=1000)\n",
    "    clf.fit(os_X_train, os_y_train)\n",
    "    y_val_pre= clf.predict(os_X_val)\n",
    "    accuracies.append(  accuracy_score(y_val_pre, os_y_val ) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the best C which has the best accuracy in validation dataset.\n",
    "accuracies.index(max(accuracies) )\n",
    "Cs[ accuracies.index(max(accuracies) ) -len(Cs) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9992743700478681\n",
      "[[85266    30]\n",
      " [   32   115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00     85296\n",
      "           1       0.79      0.78      0.79       147\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     85443\n",
      "   macro avg       0.90      0.89      0.89     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Class_weight and C are decided. Test the model in test dataset.\n",
    "y_test_pre= clf.predict(X_test)\n",
    "print( accuracy_score(y_test_pre, y_test ) ) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_pred= clf.predict(X_test)\n",
    "my_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(my_confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the coefficients, use them to build our own logistic classifier in Flink \n",
    "w= clf.coef_.reshape( (30, 1) )\n",
    "b= clf.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
