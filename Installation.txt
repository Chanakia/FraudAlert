ubuntu 18
Flink 1.4.2;   scala 2.11  hadoop 2.7, released   3/2018
kafka 1.1.0, scala 2.12,  released March 28, 2018
PostgreSQL


flink 1.8.0, scala 2.11
kafka 2.2.1, kafka 2.11. http://mirror.cogentco.com/pub/apache/kafka/2.2.1/kafka_2.11-2.2.1.tgz
flink 1.8.0, scala 2.12 ()


MASTER NODE:
    Hostname:   ip-10-0-0-10
    Public DNS: ec2-34-216-183-178.us-west-2.compute.amazonaws.com

WORKER NODE:
    Hostname:   ip-10-0-0-8
    Public DNS: ec2-52-25-69-246.us-west-2.compute.amazonaws.com

WORKER NODE:
    Hostname:   ip-10-0-0-9
    Public DNS: ec2-54-68-103-223.us-west-2.compute.amazonaws.com

Kafka 
kafka_2.12-1.1.0.jar
Scala 2.12, kafka 1.1
    
***********************************
#Set up EC2 and ssh connection
#Connect to EC2 instance from ssh:
chmod 400 /path_to_key/my_key.pem (make sure your private key file is not publicly viewable)
ssh -i ~/.ssh/xiaoling-IAM-keypair.pem ubuntu@^C  (Added by Shri)
ssh -i ~/.ssh/xiaoling-IAM-keypair.pem ubuntu@ec2-52-24-207-226.us-west-2.compute.amazonaws.com

******************************
#Install Flink

Run flink installed by pegasus:
start Hadoop, flink with pegasus
eval `ssh-agent -s`
peg fetch Flink_Master
peg service Flink_Master hadoop start
peg service Flink_Master flink start
peg service Flink_Master flink stop
peg service Flink_Master hadoop stop

./bin/flink run ../src/quickstart.jar
./bin/flink run ./examples/batch/WordCount.jar 
./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result
-j,--jarfile <jarfile>   Flink program JAR file.

#Install Java
sudo apt update
sudo apt install openjdk-8-jdk
java -version

#Setting java Path
Many frameworks require you to have the JAVA_HOME variable set - this video shows you how to do it.
Examples: Flink, Spark.


ubuntu@ip-10-0-0-7:~$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ubuntu@ip-10-0-0-7:~$ echo $JAVA_HOME
/usr/lib/jvm/java-8-openjdk-amd64
ubuntu@ip-10-0-0-7:~$ export PATH=$PATH:$JAVA_HOME/bin
ubuntu@ip-10-0-0-7:~$ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin

#Set up the passwordless 

#Install Flink, run the Flink UI, run the WordCount.jar
$ cd ~/Downloads        # Go to download directory
$ tar xzf flink-*.tgz   # Unpack the downloaded archive
$ cd flink-1.8.0
$ ./bin/start-cluster.sh  # Start Flink
$ ./bin/stop-cluster.sh

#Run the WordCount example to test the Flink
./bin/flink run ../src/quickstart.jar
./bin/flink run ./examples/batch/WordCount.jar 
./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result

#How to build my own project with Maven (Java or Scala): 
javac -cp .:flink-dist_2.11-1.8.0.jar:log4j-1.2.17.jar:slf4j-log4j12-1.7.15.jar WordCount.java #With dependencies in jar #In a naive way
jar -cvf wordCount.jar *.class

1) Install Maven
sudo apt update
sudo apt install maven
mvn -version
2) Maven Dependencies,  pom.xml


3) Build Flink from source (This is not necessary, this is sucessful in my computer)
git clone https://github.com/apache/flink
mvn clean install -DskipTests
mvn clean install -DskipTests -Dfast

4) Download the template Maven template and build to jar: Java
curl https://flink.apache.org/q/quickstart.sh | bash -s 1.8.0
tree quickstart/
mvn clean package (inside the root directory)

curl https://flink.apache.org/q/quickstart-scala.sh | bash -s 1.8.0 #maven, scala
tree quickstart/
mvn clean package

sbt new tillrohrmann/flink-project.g8 #sbt, scala
sbt clean assembly #Build the project


****************************
Pegasus
Use pegasus to install flink

$ peg install <cluster-name> ssh
$ peg install <cluster-name> aws
$ peg install <cluster-name> environment
peg sshcmd-cluster Flink_Master "sudo apt install -y bc"



Kafka

#Start and stop Kafka in the Kafka node 
sudo /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &
sudo /usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties &

sudo /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties \
  --override delete.topic.enable=true \
  --override broker.id=100 \
  --override log.dirs=/tmp/kafka-logs-100 \
  --override port=9192  #Start a Kafka where we can delete a topic

Start zookeeper and Kafka from pegasus
eval `ssh-agent -s`
peg fetch KafkaBroker
peg service KafkaBroker zookeeper start
peg service KafkaBroker kafka start
peg service KafkaBroker kafka stop
peg service KafkaBroker zookeeper stop

#Start a topic
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 2 --topic test-topic
/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181 #Print the list of topics
/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test-topic #Check the state of the topic
/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 #Describe the topics and see who takes card of which partition
/usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic #Publish some message
/usr/local/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic test-topic #define a local consumer


/usr/local/kafka/bin/kafka-topics.sh --delete  --zookeeper localhost:2181 --topic price_data_part4
#delete a topic

#Kill a node
ps aux | grep server.properties #get the node's characteristics
sudo kill -9 10121

#Install Kafka manager


#Producer, Costumer and Kafka
bash spawn_kafka_streams.sh









